# ğŸ§  DEEP LEARNING â€“ COMPLETE NOTES (DAY 01)

---

## PART 1: Neural Network Parameter Initialization

### Why Initialization Matters

Initialization controls:

* **Forward signal flow**
* **Backward gradient flow**
* **Training stability**
* **Speed of convergence**

Bad initialization leads to:

* Vanishing gradients
* Exploding gradients
* Symmetry problems (no learning)

> Initialization must **break symmetry** and **preserve variance**.

---

## Zero Initialization

### What it is

* All weights initialized to **0**
* Biases initialized to **0**

### Why it fails

* All neurons compute the **same output**
* All gradients are **identical**
* Neurons never **specialize**
* Learning collapses due to **symmetry**

### Important distinction

* âœ… Bias = 0 â†’ fine
* âŒ Weights = 0 â†’ fatal

> Zero initialization prevents learning because it does not break symmetry.

---

## Random Initialization (Naive)

### Idea

* Initialize weights randomly (small values)

### Problem

* Too small â†’ vanishing gradients
* Too large â†’ exploding gradients
* No control over variance

> Random alone is **not sufficient** for deep networks.

---

## Xavier (Glorot) Initialization

![Xavier Initialization](https://365datascience.com/resources/blog/k3hghmdc8lb-xavier-initialization-9.jpeg)

![Xavier Formula](https://miro.medium.com/1%2Az7Dpo68uOaKjaxbzxVYJ3g.png)

### Designed for

* **Sigmoid**
* **Tanh**

### Key assumption

* Activations are **symmetric around zero**

### Formula

$$\text{Var}(W) = \frac{1}{N_{in}}$$

### Goal

* Keep variance **constant across layers**
* Prevent shrinkage or explosion of signals

### Why it works

* Sigmoid and tanh squash symmetrically
* Variance is preserved both forward and backward

### Limitation

* **Suboptimal for ReLU**
* ReLU zeros out half the activations â†’ signal shrinkage

---

## Kaiming (He) Initialization

![Kaiming He Initialization](https://www.oreilly.com/api/v2/epubs/urn%3Aorm%3Abook%3A9781785880360/files/assets/7cb3fa40-3f2c-4555-a12e-6d36e10cab45.png)

![ReLU Activation](https://res.cloudinary.com/jithinjayan1993/image/upload/v1595043720/Is-Relu-Dead/1_eXsCUjn0UVJbnGV1EXvFbg_m3dcee.png)

### Designed for

* **ReLU**
* **Leaky ReLU**

### Key insight

* ReLU kills ~50% of activations (outputs zero)

### Formula

$$\text{Var}(W) = \frac{2}{N_{in}}$$

### Why multiply by 2

* Compensates for lost variance
* Maintains signal strength in deep networks

### Why it's critical for CNNs

* CNNs are **deep**
* CNNs almost always use **ReLU**
* Without Kaiming â†’ gradients decay layer by layer

---

## Initialization Summary (Very Important)

| Activation | Initialization |
| ---------- | -------------- |
| Sigmoid    | Xavier         |
| Tanh       | Xavier         |
| ReLU       | Kaiming        |
| Deep CNNs  | Kaiming        |

> **Activation decides initialization**

---

# PART 2: Optimizers â€“ Adam vs AdamW

---

## Problems with Vanilla Gradient Descent

* Same learning rate for all parameters
* Sensitive to curvature
* Slow convergence
* Gets stuck near saddle points

---

## Adam Optimizer

![Adam Optimizer Animation](https://blog.paperspace.com/content/images/size/w350/2018/06/optimizers7.gif)

![Adam vs RMSProp](https://www.researchgate.net/profile/Henrique-Donancio-2/publication/384973999/figure/fig2/AS%3A11431281284205137%401729136337503/A-comparison-between-Adam-and-RMSProp-with-momentum-using-either-DQN-or-LRRL.ppm)

### Adam = Momentum + RMSProp

Tracks:

1. **First moment** â†’ mean of gradients (momentum)
2. **Second moment** â†’ variance of gradients (adaptive scaling)

### Benefits

* Adaptive learning rate per parameter
* Fast convergence
* Robust to noisy gradients
* Works well in practice

### Hidden Problem

> **Adam + L2 regularization â‰  true weight decay**

* L2 gets mixed with gradient scaling
* Leads to poor generalization
* Can converge fast but generalize badly

---

## AdamW Optimizer

![AdamW vs Adam](https://miro.medium.com/1%2AMkKinCkJxO_8TMYVmK1E2A.png)

![AdamW Performance](https://www.researchgate.net/publication/342733846/figure/fig6/AS%3A910503777951745%401594092425385/Comparison-of-performance-between-Adam-and-AdamW-optimizers-a-Training-IoU-score-b.jpg)

### Key Fix

* **Decouples weight decay from loss**
* Applies decay **directly to weights**

### Why AdamW is better

* True weight decay
* Better regularization
* Better generalization
* Stable training

### Modern Default

* Transformers
* CNNs
* Vision Transformers (ViT)
* Large-scale deep learning

---

## Optimizer Summary

| Optimizer | Use                                |
| --------- | ---------------------------------- |
| SGD       | Theory, small models               |
| Adam      | Fast convergence                   |
| **AdamW** | **Modern default (best practice)** |

> **Adam adapts gradients, AdamW fixes regularization**

---

# PART 3: Convolutional Neural Networks (CNNs)

---

## Why CNNs Exist

Fully Connected Networks:

* Ignore spatial structure
* Too many parameters
* Poor generalization for images

CNNs exploit:

* Locality
* Weight sharing
* Translation equivariance

---

## Convolution Operation

![Convolution Animation](https://miro.medium.com/0%2A1KEew1smVFM7AnA3)

![Convolution Math](https://miro.medium.com/1%2AixuhX9vaf1kUQTWicVYiyg.png)

### What happens

* Small kernel slides over image
* Dot product â†’ feature map

### Key Parameters

* Kernel size (e.g. 3Ã—3)
* Stride
* Padding

### Why convolution works

* Detects local patterns (edges, textures)
* Same filter â†’ same feature everywhere

---

## Weight Sharing

### Meaning

* Same filter used across all spatial locations

### Benefits

1. Huge reduction in parameters
2. Acts as regularization
3. Prevents memorization of noise
4. Enables **translation equivariance**

> If an object moves, its feature map moves the same way.

---

## Pooling Layer

![Max vs Average Pooling](https://www.researchgate.net/publication/333593451/figure/fig2/AS%3A765890261966848%401559613876098/llustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max.png)

![Pooling Diagram](https://blog.paperspace.com/content/images/2023/12/Screenshot-2023-12-04-at-3.37.12-PM.png)

### Purpose

* Reduce spatial dimensions
* Make features robust to small shifts
* Reduce computation

### Types

* Max Pooling (most common)
* Average Pooling

### Why Max Pooling is preferred

* Preserves strong activations
* Highlights edges and corners
* Answers: *"Is this feature present?"*

---

## ReLU Placement (Very Important)

### Order

**Convolution â†’ ReLU â†’ Pooling**

### Why

* Convolution: linear feature detection
* ReLU: introduces non-linearity + sparsity
* Pooling: summarizes meaningful activations

> Pooling without ReLU would downsample weak linear signals.

---

## CNN Architecture (End-to-End)

![CNN Architecture](https://ik.imagekit.io/upgrad1/abroad-images/imageCompo/images/unnamed8PDPDZ_1_1ZBHFR.webp)

![CNN Flow](https://images.openai.com/static-rsc-3/0N3yrmKDAE2B8Yp2CUX5Z37SDmfJDNl3mWkIer3JGdUR2FsJP_Rf7dp5bbg4ur_pjRnAqPFr3zynScPUfECee8HALMyjqB8Kev-a9vjtV2s?purpose=fullsize&v=1)

### Typical Flow

```
Input
â†’ Conv â†’ ReLU
â†’ Pool
â†’ Conv â†’ ReLU
â†’ Pool
â†’ Flatten
â†’ Fully Connected
â†’ Softmax
```

### Hierarchy of features

* Early layers â†’ edges
* Middle layers â†’ shapes
* Deep layers â†’ objects

---

## Big Picture Mental Model

> **CNN = Feature Extractor + Classifier**

* Conv + ReLU + Pool â†’ Feature extraction
* Fully connected layers â†’ Decision making

---

# ğŸ”‘ FINAL ONE-PAGE MEMORY SUMMARY

* Zero init fails due to symmetry
* Xavier â†’ sigmoid/tanh
* Kaiming â†’ ReLU / deep CNNs
* Adam is fast but weak regularization
* AdamW is modern default
* CNNs use weight sharing to reduce overfitting
* Max pooling preserves sharp features
* ReLU before pooling is essential

---

## ğŸ’­ Ideas & Thoughts

- Practice implementing Xavier and Kaiming init from scratch
- Compare Adam vs AdamW on a small CNN project
- Visualize feature maps at different CNN layers

---

## ğŸ”– Important Reminders

- **Activation decides initialization** - memorize the table!
- Always use AdamW for modern projects
- Conv â†’ ReLU â†’ Pool order is critical

---

## â“ Questions to Explore

- What happens if you use Xavier with ReLU?
- How does batch normalization interact with initialization?
- When would you prefer Average Pooling over Max Pooling?

---

*Day 1 of Deep Learning Journey â€“ Captured everything that matters!* ğŸš€
